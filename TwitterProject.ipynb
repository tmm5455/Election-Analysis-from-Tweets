{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tanvi\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import decomposition\n",
    "from twython import TwythonStreamer\n",
    "from datetime import datetime\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#variables and functions\n",
    "tweets = []\n",
    "x = random.randrange(5,10)\n",
    "dt = str(datetime.now().time())[:8].translate(None,':')\n",
    "kywrds=['Obama','Trump','Clinton']\n",
    "jsonFiles = []\n",
    "polObama,subObama = [],[]\n",
    "polTrump,subTrump = [],[]\n",
    "polClinton,subClinton=[],[]\n",
    "obama,trump,clinton = '','',''\n",
    "obamaCorpus,trumpCorpus,clintonCorpus=[],[],[]\n",
    "jsonObamaFiles,jsonTrumpFiles,jsonClintonFiles = [],[],[]\n",
    "def strip_non_ascii(string):\n",
    "    ''' Returns the string without non ASCII characters'''\n",
    "    stripped = (c for c in string if 0 < ord(c) < 127)\n",
    "    return ''.join(stripped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Tanvi\\\\Downloads\\\\Project'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('C:/Users/Tanvi/Downloads/Project')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Streaming 200 tweets per connection\n",
    "class MyStreamer(TwythonStreamer):\n",
    "\n",
    "    def on_success(self, data):\n",
    "        if 'text' in data:\n",
    "            if data['lang'] == 'en':\n",
    "                tweets.append(data)\n",
    "                print 'received tweet #', len(tweets), data['text'][:100].encode('utf-8')\n",
    "        \n",
    "        #200 tweets per connection to be stored in a json file\n",
    "        if len(tweets) >= 200:\n",
    "            self.store_json()\n",
    "            self.disconnect()\n",
    "        \n",
    "    def on_error(self, status_code, data):\n",
    "        print status_code, data\n",
    "        self.store_json()        \n",
    "        self.disconnect()\n",
    "\n",
    "    def store_json(self):\n",
    "        with open('tweet_stream_{}_{}_{}.json'.format(dt, kywrds[wrd], i), 'w') as f:\n",
    "            json.dump(tweets, f, indent=4)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open('tanvi_twitter_credentials.json', 'r') as f:\n",
    "        credentials = json.load(f)\n",
    "\n",
    "    # create your own app to get consumer key and secret\n",
    "    CONSUMER_KEY = credentials['CONSUMER_KEY']\n",
    "    CONSUMER_SECRET = credentials['CONSUMER_SECRET']\n",
    "    ACCESS_TOKEN = credentials['ACCESS_TOKEN']\n",
    "    ACCESS_TOKEN_SECRET = credentials['ACCESS_TOKEN_SECRET'] \n",
    "    os.chdir('./json')\n",
    "    for wrd in range(len(kywrds)):\n",
    "        for i in range(1,51):\n",
    "            stream  = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)            \n",
    "            stream.statuses.filter(track=kywrds[wrd])\n",
    "            tweets=[]\n",
    "            time.sleep(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets collected:30200\n",
      "\n",
      "Polarity:\n",
      "Obama:0.0339125475215 Trump:0.059704368941 Clinton:0.0207804921994\n",
      "\n",
      "Subjectivity:\n",
      "Obama:0.312074594767 Trump:0.312719156887 Clinton:0.325309725985\n",
      "\n",
      "Total no. of tweets:\n",
      "Obama:10000 Trump:10200 Clinton:10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sentiment Analysis\n",
    "jsonObamaFiles,jsonTrumpFiles,jsonClintonFiles = [],[],[]\n",
    "filepath = 'C:/Users/Tanvi/Downloads/Project/json'\n",
    "for files in os.listdir(filepath):\n",
    "    if files.endswith('json') and 'Obama' in files:\n",
    "        jsonObamaFiles.append(files)\n",
    "    elif files.endswith('json') and 'Trump' in files:\n",
    "        jsonTrumpFiles.append(files)\n",
    "    elif files.endswith('json') and 'Clinton' in files:\n",
    "        jsonClintonFiles.append(files)\n",
    "os.chdir(filepath)\n",
    "for filename in jsonObamaFiles:\n",
    "    infile = open(filename).read()\n",
    "    content = json.loads(infile)\n",
    "    for i in range(len(content)):        \n",
    "        tweet = strip_non_ascii(content[i]['text'])\n",
    "        obama += tweet + '\\n'\n",
    "        senObama = TextBlob(tweet)\n",
    "        polObama.append(senObama.sentiment.polarity)\n",
    "        subObama.append(senObama.sentiment.subjectivity)\n",
    "    \n",
    "for filename in jsonTrumpFiles:\n",
    "    infile = open(filename).read()\n",
    "    content = json.loads(infile)\n",
    "    for i in range(len(content)):        \n",
    "        tweet = strip_non_ascii(content[i]['text'])\n",
    "        trump += tweet + '\\n' \n",
    "        senTrump = TextBlob(tweet)\n",
    "        polTrump.append(senTrump.sentiment.polarity)\n",
    "        subTrump.append(senTrump.sentiment.subjectivity)\n",
    "    \n",
    "for filename in jsonClintonFiles:\n",
    "    infile = open(filename).read()\n",
    "    content = json.loads(infile)\n",
    "    for i in range(len(content)):        \n",
    "        tweet = strip_non_ascii(content[i]['text'])\n",
    "        clinton += tweet + '\\n' \n",
    "        senClinton = TextBlob(tweet)\n",
    "        polClinton.append(senClinton.sentiment.polarity)\n",
    "        subClinton.append(senClinton.sentiment.subjectivity)\n",
    "\n",
    "\n",
    "print('Total number of tweets collected:{}\\n'.format((len(jsonObamaFiles)+len(jsonTrumpFiles)+len(jsonClintonFiles))*len(content)))\n",
    "print('Polarity:\\nObama:{} Trump:{} Clinton:{}\\n'.format(np.mean(polObama),np.mean(polTrump),np.mean(polClinton)))\n",
    "print('Subjectivity:\\nObama:{} Trump:{} Clinton:{}\\n'.format(np.mean(subObama),np.mean(subTrump),np.mean(subClinton)))\n",
    "print('Total no. of tweets:\\nObama:{} Trump:{} Clinton:{}\\n'.format(len(subObama),len(subTrump),len(subClinton)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Storing tweets in files\n",
    "with open('ObamaTweets.txt', 'w') as f2:\n",
    "    f2.write(obama)\n",
    "with open('TrumpTweets.txt', 'w') as f2:\n",
    "    f2.write(trump)\n",
    "with open('ClintonTweets.txt', 'w') as f2:\n",
    "    f2.write(clinton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#WordClouds\n",
    "os.chdir('C:/Users/Tanvi/Downloads/Project')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append(u'https')\n",
    "stopwords.append(u'http')\n",
    "stopwords.append(u'rt')\n",
    "\n",
    "wcObama = WordCloud(max_font_size=40,stopwords=stopwords).generate(obama.lower())\n",
    "wcObama.to_file(\"obama1.png\")\n",
    "plt.figure()\n",
    "plt.imshow(wcObama)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "wcTrump = WordCloud(max_font_size=40,stopwords=stopwords).generate(trump.lower())\n",
    "wcTrump.to_file(\"trump1.png\")\n",
    "plt.figure()\n",
    "plt.imshow(wcTrump)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "wcClinton = WordCloud(max_font_size=40,stopwords=stopwords).generate(clinton.lower())\n",
    "wcClinton.to_file(\"clinton1.png\")\n",
    "plt.figure()\n",
    "plt.imshow(wcClinton)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Streaming tweets for specific locations\n",
    "kywrds=['SFO','NYC','DAL','LA','CHI','DC','ATL']\n",
    "locations = ['-122.75,36.8,-121.75,37.8',\n",
    "             '-74,40,-73,41',\n",
    "             '-96.89,32.66,-96.67,32.87',\n",
    "             '-118.79,32.64,-116.97,34.90',\n",
    "             '-87.66,41.83,-87.63,42.01',\n",
    "             '-77.03,38.86,-76.97,38.94',\n",
    "             '-85.02,33.37,-83.81,34.22']\n",
    "os.chdir('./json1')\n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    for wrd in range(7):\n",
    "        for i in range(1,11):\n",
    "            stream  = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)            \n",
    "            stream.statuses.filter(locations=locations[wrd])\n",
    "            tweets=[]\n",
    "            time.sleep(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets collected:14000\n",
      "\n",
      "Polarity:\n",
      "Obama:0.0346672620324 Trump:0.080184319246 Clinton:0.0251427085986\n",
      "\n",
      "Subjectivity:\n",
      "Obama:0.374303683738 Trump:0.357106461748 Clinton:0.421674199983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sentiment Analysis for those locations\n",
    "jsonFiles = []\n",
    "polObama,subObama = [],[]\n",
    "polTrump,subTrump = [],[]\n",
    "polClinton,subClinton=[],[]\n",
    "obama,trump,clinton = '','',''\n",
    "kywrds=['ATL','CHI','DAL','DC','LA','SFO','NYC']\n",
    "filepath = 'C:/Users/Tanvi/Downloads/Project/json1'\n",
    "\n",
    "os.chdir(filepath)\n",
    "for files in os.listdir(filepath):\n",
    "    if files.endswith(\".json\") and ('Obama' not in files) and ('Trump' not in files ) and ('Clinton' not in files):\n",
    "        jsonFiles.append(files)\n",
    "jsonFiles.sort()\n",
    "kywrds.sort()\n",
    "j,k=0,0\n",
    "for filename in jsonFiles:\n",
    "    infile = open(filename).read()\n",
    "    content = json.loads(infile)\n",
    "    #to be run for the number of tweets collected per json file   \n",
    "    k += 1\n",
    "    for i in range(len(content)):        \n",
    "        token = content[i]['text']\n",
    "        \n",
    "        if 'obama' in token.lower():\n",
    "            token = strip_non_ascii(token)\n",
    "            obama += token + '\\n'\n",
    "            senObama = TextBlob(token)\n",
    "            polObama.append(senObama.sentiment.polarity)\n",
    "            subObama.append(senObama.sentiment.subjectivity)\n",
    "        elif ('trump' or 'donald') in token.lower():\n",
    "            token = strip_non_ascii(token)\n",
    "            trump += token + '\\n'\n",
    "            senTrump = TextBlob(token)\n",
    "            polTrump.append(senTrump.sentiment.polarity)\n",
    "            subTrump.append(senTrump.sentiment.subjectivity)\n",
    "        elif ('clinton' or 'hillary') in token.lower():\n",
    "            token = strip_non_ascii(token)\n",
    "            clinton += token + '\\n'\n",
    "            senClinton = TextBlob(token)\n",
    "            polClinton.append(senClinton.sentiment.polarity)\n",
    "            subClinton.append(senClinton.sentiment.subjectivity)\n",
    "    with open('{}.txt'.format(kywrds[j]), 'a') as f2:\n",
    "        f2.write(obama)\n",
    "        f2.write(trump)\n",
    "        f2.write(clinton)\n",
    "    if k % 10 == 0:\n",
    "        j += 1\n",
    "print('Total number of tweets collected:{}\\n'.format(len(jsonFiles)*len(content)))\n",
    "print('Polarity:\\nObama:{} Trump:{} Clinton:{}\\n'.format(np.mean(polObama),np.mean(polTrump),np.mean(polClinton)))\n",
    "print('Subjectivity:\\nObama:{} Trump:{} Clinton:{}\\n'.format(np.mean(subObama),np.mean(subTrump),np.mean(subClinton)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Wordclouds for each candidate of each location\n",
    "for wrd in kywrds:\n",
    "    infile = open('{}.txt'.format(wrd)).read()\n",
    "    if 'obama' in infile.lower():\n",
    "        wcObama = WordCloud(max_font_size=40,stopwords=stopwords).generate(infile.lower())\n",
    "        wcObama.to_file(\"obama{}.png\".format(wrd))        \n",
    "        plt.figure()\n",
    "        plt.imshow(wcObama)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No tweets of obama in {}'.format(wrd))\n",
    "    if 'trump' in infile.lower(): \n",
    "        wcTrump = WordCloud(max_font_size=40,stopwords=stopwords).generate(infile.lower())\n",
    "        wcTrump.to_file(\"trump{}.png\".format(wrd))        \n",
    "        plt.figure()\n",
    "        plt.imshow(wcTrump)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No tweets of trump in {}'.format(wrd))\n",
    "    if 'clinton' in infile.lower():\n",
    "        wcClinton = WordCloud(max_font_size=40,stopwords=stopwords).generate(infile.lower())\n",
    "        wcClinton.to_file(\"clinton{}.png\".format(wrd))\n",
    "        plt.figure()\n",
    "        plt.imshow(wcClinton)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No tweets of clinton in {}'.format(wrd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Preparing corpus for NMF\n",
    "jsonFiles = []\n",
    "polObama,subObama = [],[]\n",
    "polTrump,subTrump = [],[]\n",
    "polClinton,subClinton=[],[]\n",
    "obama,trump,clinton = '','',''\n",
    "def parse_text(text): #funtion to clean tweet data\n",
    "    text = re.sub(r\"http\\S+\", \"\", text) \n",
    "    text = re.sub(r\"@\\S+\", \"\", text)    \n",
    "    text = re.sub(r\"#\\S+\", \"\", text)    \n",
    "    text = re.sub(r'[-.#/?@🔪!,\":;()]',' ',text.lower().replace('\\n', ' '))\n",
    "    text = re.sub(r'[\\']','',text) \n",
    "    text = re.sub(r\"\\d\", \"\", text) \n",
    "    text = re.sub(r'https|2016|hillary|clinton|hillaryclinton|donald|trump|Trump|donaldtrump|obama2016|barackobama|barack|obama|https|2016|rt','',text)\n",
    "    text = ' '.join(text.split())  \n",
    "    return text\n",
    "\n",
    "os.chdir('C:/Users/Tanvi/Downloads/Project/json')\n",
    "jsonObamaFiles,jsonObamaFiles,jsonClintonFiles=[],[],[]\n",
    "for files in os.listdir('../json'):\n",
    "    if files.endswith('json') and 'Obama' in files:\n",
    "        jsonObamaFiles.append(files)\n",
    "    elif files.endswith('json') and 'Trump' in files:\n",
    "        jsonTrumpFiles.append(files)\n",
    "    elif files.endswith('json') and 'Clinton' in files:\n",
    "        jsonClintonFiles.append(files)\n",
    "for filename in jsonObamaFiles:\n",
    "    infile = open(filename).read()\n",
    "    content = json.loads(infile)\n",
    "    for i in range(len(content)):        \n",
    "        tweet = parse_text(content[i]['text'])\n",
    "        obama += tweet + '\\n'\n",
    "    obamaCorpus.append(obama)\n",
    "    obama=''\n",
    "for filename in jsonTrumpFiles:\n",
    "    infile = open(filename).read()\n",
    "    content = json.loads(infile)\n",
    "    for i in range(len(content)):        \n",
    "        tweet = parse_text(content[i]['text'])\n",
    "        trump += tweet + '\\n'\n",
    "    trumpCorpus.append(trump)\n",
    "    trump = ''\n",
    "for filename in jsonClintonFiles:\n",
    "    infile = open(filename).read()\n",
    "    content = json.loads(infile)\n",
    "    for i in range(len(content)):        \n",
    "        tweet = parse_text(content[i]['text'])\n",
    "        clinton += tweet + '\\n'\n",
    "    clintonCorpus.append(clinton)\n",
    "    clinton=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4138)\n",
      "Topic 0: biden president joe says past password losers know dont whats\n",
      "Topic 1: mothers kids forced apologize whos like president look biden yorker\n",
      "Topic 2: issued warning sick rioters completely just media silent president amp\n",
      "Topic 3: biden just president left needs bag bathroom cheetos case powder\n",
      "Topic 4: cutest couple michelle biden president amp just joe disappear white\n",
      "Topic 5: yeah presi daquan outta officer car step used biden president\n",
      "Topic 6: owns phone president biden just joe degree says dont shares\n",
      "Topic 7: predicted baba vanga president teacher gorilla fired calling american biden\n",
      "Topic 8: fundamental just stay photos years difference biden amp joe merkel\n",
      "Topic 9: president news fake freedoms problem amp democratic says located thankful\n",
      "Topic 10: biden president just world merkel amp joe white michelle decal\n",
      "Topic 11: biden president amp joe right new genocide just years dont\n",
      "Topic 12: biden president wwc joe amp ryan like russia years problems\n",
      "Topic 13: president biden joe amp protesters white just pres speaking news\n",
      "Topic 14: biden president amp joe just white vs dont news vote\n",
      "Topic 15: biden just president amp years joe stay merkel new advisor\n",
      "Topic 16: president biden joe just amp minus stalking splits merkel gang\n",
      "Topic 17: biden president amp merkel just world russia protesters job like\n",
      "Topic 18: biden joe disappear president amp dont just entic people ambulance\n",
      "Topic 19: biden amp president joe just like election walks fake left\n",
      "(51, 5088)\n",
      "Topic 0: university million president settlement case fraud elect general amp new\n",
      "Topic 1: president grab crotch science meet attention discuss kanye sessions voted\n",
      "Topic 2: hamilton motherfucker sand arab nigger terrorist fucking amp president just\n",
      "Topic 3: enjoying youall bond nasty bird loss market enjoy trillion hope\n",
      "Topic 4: nude published photos post new york melania kanye million amp\n",
      "Topic 5: university million fraud kanye starbucks settlement lawsuits settles president just\n",
      "Topic 6: president kanye just people university ism selecting bannon warmonger readers\n",
      "Topic 7: hits girlfriend university return kanye gets watch president just guy\n",
      "Topic 8: president million university starbucks general attorney iraqis fear backfire white\n",
      "Topic 9: president nearing kanye meet grab attention libeies racist crotch discuss\n",
      "Topic 10: hamilton discrimination order republican suppoers barron protest outrageous doj starbucks\n",
      "Topic 11: kanye starbucks amp president vote people im voted fraud cup\n",
      "Topic 12: million university president pay just fraud elect settlement settle attorney\n",
      "Topic 13: president kanye people university million elect settlement pay mike new\n",
      "Topic 14: hamilton don cast apologize amp disagrees criticize president safe free\n",
      "Topic 15: kanye university case million settlement sessions new president breaking mike\n",
      "Topic 16: president university general attorney settlement million elect just saturday racist\n",
      "Topic 17: upside aist president turn kanye starbucks university asked response sessions\n",
      "Topic 18: university million kanye president sunday major attorney general new settlement\n",
      "Topic 19: million university kanye settlement amp starbucks president case hide announces\n",
      "(50, 3866)\n",
      "Topic 0: campaign like hilly growth austerity administrations fiscal thousands looks west\n",
      "Topic 1: mall loses naughty girl santa list telling job hilly news\n",
      "Topic 2: suns pissed xx capri gets election campaign amp impeach voted\n",
      "Topic 3: suppoer feminist racist hilly foundation vote wont crimes investigating dead\n",
      "Topic 4: hilly news bran hows adveising hit piece thank network free\n",
      "Topic 5: patriots afi fest berg peter director day suppoers says news\n",
      "Topic 6: hispanic worse probably yes did xx voters amp campaign suns\n",
      "Topic 7: hilly noh traffic campaign nov saturday ale raises avenue like\n",
      "Topic 8: thunderstorm howard boone severe warning hilly montgomery carroll pm dead\n",
      "Topic 9: lt campaign like thousands west donated dollars looks kanye vote\n",
      "Topic 10: news hilly fake network vote santa naughty mall girl popular\n",
      "Topic 11: hilly campaign amp million election vote foundation president cleveland people\n",
      "Topic 12: campaign kanye west election vote donated news infrastructure amp pr\n",
      "Topic 13: hilly college electoral ll santa steal general president just job\n",
      "Topic 14: news job hilly predator old election dragging woke jeff santa\n",
      "Topic 15: hilly vote foundation news house deficit million wh infrastructure pr\n",
      "Topic 16: college electoral president electors amp fo minimum december make hilly\n",
      "Topic 17: hilly amp million election consider vote rep dead investigating girl\n",
      "Topic 18: vote campaign news million just election hilly like college amp\n",
      "Topic 19: questionable election foundation tweets campaign security national adviser hilly dead\n"
     ]
    }
   ],
   "source": [
    "#Running NMF for tweets of each candidate\n",
    "def corp(corpText):\n",
    "    dtm = vectorizer.fit_transform(corpText) #obamaCorpus is a list of files having tweets.\n",
    "    print(dtm.shape)\n",
    "    \n",
    "    num_terms = len(vectorizer.vocabulary_)\n",
    "    terms = [\"\"] * num_terms\n",
    "    for term in vectorizer.vocabulary_.keys():\n",
    "    \tterms[ vectorizer.vocabulary_[term] ] = term\n",
    "    \n",
    "    num_topics = 20\n",
    "    num_top_words = 10\n",
    "    clf = decomposition.NMF(n_components = num_topics, random_state=2)\n",
    "    doctopic = clf.fit_transform(dtm)\n",
    "    \n",
    "    topic_words = []\n",
    "    for topic in clf.components_:\n",
    "        word_idx = np.argsort(topic)[::-1][0:num_top_words]\n",
    "        #print (word_idx)    \n",
    "        topic_words.append([terms[i] for i in word_idx])\n",
    "    \n",
    "    for t in range(len(topic_words)):\n",
    "        print(\"Topic {}: {}\".format(t, ' '.join(topic_words[t][:20])))\n",
    "    topic_words = []\n",
    "    \n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=2)\n",
    "corp(obamaCorpus)\n",
    "corp(trumpCorpus)\n",
    "corp(clintonCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.090*\"obama\" + 0.048*\"trump\" + 0.031*\"russia\" + 0.029*\"go\" + 0.028*\"now\" + 0.027*\"stand\" + 0.019*\"presid\" + 0.019*\"say\" + 0.018*\"hope\" + 0.018*\"pre\"'), (1, u'0.111*\"trump\" + 0.071*\"obama\" + 0.046*\"protest\" + 0.041*\"anti\" + 0.023*\"march\" + 0.019*\"two\" + 0.018*\"foxnew\" + 0.012*\"chief\" + 0.012*\"advisor\" + 0.011*\"s\"'), (2, u'0.053*\"yeah\" + 0.043*\"need\" + 0.037*\"hillari\" + 0.037*\"biden\" + 0.033*\"case\" + 0.033*\"obama\" + 0.028*\"powder\" + 0.028*\"no\" + 0.026*\"offic\" + 0.019*\"pardon\"'), (3, u'0.060*\"obama\" + 0.053*\"trump\" + 0.020*\"presid\" + 0.018*\"put\" + 0.018*\"crime\" + 0.017*\"potu\" + 0.017*\"fuck\" + 0.015*\"car\" + 0.014*\"america\" + 0.013*\"hate\"'), (4, u'0.155*\"biden\" + 0.042*\"say\" + 0.034*\"obama\" + 0.034*\"loser\" + 0.030*\"left\" + 0.024*\"bag\" + 0.023*\"whisper\" + 0.023*\"cheeto\" + 0.023*\"bathroom\" + 0.020*\"hand\"'), (5, u'0.078*\"obama\" + 0.018*\"vote\" + 0.016*\"michel\" + 0.014*\"pleas\" + 0.014*\"call\" + 0.013*\"pardon\" + 0.013*\"democrat\" + 0.013*\"republican\" + 0.012*\"2\" + 0.012*\"clinton\"'), (6, u'0.082*\"obama\" + 0.059*\"just\" + 0.032*\"nation\" + 0.023*\"tweet\" + 0.022*\"4\" + 0.022*\"secur\" + 0.019*\"new\" + 0.018*\"thing\" + 0.018*\"presid\" + 0.018*\"trumpleak\"'), (7, u'0.048*\"obama\" + 0.030*\"s\" + 0.026*\"said\" + 0.023*\"silent\" + 0.022*\"good\" + 0.018*\"presid\" + 0.018*\"peopl\" + 0.017*\"will\" + 0.016*\"wikileak\" + 0.016*\"isn\"'), (8, u'0.048*\"obama\" + 0.024*\"realdonaldtrump\" + 0.022*\"make\" + 0.020*\"m\" + 0.017*\"biden\" + 0.016*\"sh\" + 0.016*\"gonna\" + 0.014*\"never\" + 0.014*\"11\" + 0.013*\"way\"'), (9, u'0.074*\"obama\" + 0.046*\"white\" + 0.034*\"hous\" + 0.027*\"problem\" + 0.025*\"day\" + 0.023*\"michel\" + 0.023*\"depriv\" + 0.014*\"can\" + 0.014*\"call\" + 0.013*\"propaganda\"'), (10, u'0.060*\"don\" + 0.058*\"know\" + 0.055*\"trump\" + 0.037*\"right\" + 0.032*\"obama\" + 0.023*\"elect\" + 0.020*\"piec\" + 0.018*\"remnick\" + 0.014*\"extraordinari\" + 0.012*\"guess\"'), (11, u'0.054*\"can\" + 0.054*\"get\" + 0.047*\"trump\" + 0.044*\"obama\" + 0.033*\"past\" + 0.029*\"s\" + 0.022*\"typicalglrl\" + 0.019*\"think\" + 0.016*\"run\" + 0.012*\"still\"'), (12, u'0.198*\"obama\" + 0.097*\"joe\" + 0.017*\"d\" + 0.017*\"barack_and_jo\" + 0.016*\"use\" + 0.015*\"forc\" + 0.014*\"don\" + 0.011*\"internet\" + 0.010*\"remain\" + 0.010*\"name\"'), (13, u'0.056*\"obama\" + 0.042*\"white\" + 0.039*\"vote\" + 0.039*\"black\" + 0.029*\"s\" + 0.026*\"trump\" + 0.019*\"flynn\" + 0.017*\"question\" + 0.015*\"follow\" + 0.015*\"funder\"'), (14, u'0.067*\"obama\" + 0.032*\"s\" + 0.028*\"merkel\" + 0.024*\"won\" + 0.023*\"angela\" + 0.023*\"presid\" + 0.023*\"last\" + 0.022*\"world\" + 0.020*\"liber\" + 0.016*\"may\"'), (15, u'0.068*\"obama\" + 0.027*\"presid\" + 0.027*\"time\" + 0.024*\"will\" + 0.022*\"democrat\" + 0.018*\"freedom\" + 0.014*\"rioter\" + 0.013*\"leadership\" + 0.012*\"s\" + 0.012*\"noth\"'), (16, u'0.088*\"obama\" + 0.060*\"news\" + 0.060*\"barack\" + 0.052*\"presid\" + 0.043*\"fake\" + 0.028*\"trump\" + 0.021*\"problem\" + 0.017*\"tell\" + 0.015*\"merkel\" + 0.014*\"differ\"'), (17, u'0.079*\"obama\" + 0.024*\"trump\" + 0.021*\"like\" + 0.021*\"look\" + 0.020*\"vs\" + 0.019*\"leav\" + 0.018*\"just\" + 0.014*\"presid\" + 0.013*\"voter\" + 0.013*\"expos\"'), (18, u'0.065*\"s\" + 0.057*\"biden\" + 0.052*\"obama\" + 0.032*\"password\" + 0.027*\"want\" + 0.021*\"just\" + 0.015*\"let\" + 0.014*\"give\" + 0.013*\"man\" + 0.013*\"media\"'), (19, u'0.045*\"year\" + 0.042*\"obama\" + 0.029*\"great\" + 0.024*\"8\" + 0.023*\"went\" + 0.021*\"realli\" + 0.020*\"via\" + 0.020*\"ago\" + 0.017*\"youtub\" + 0.016*\"enlighten\"')]\n",
      "[(0, u'0.034*\"s\" + 0.028*\"mike\" + 0.022*\"penc\" + 0.020*\"said\" + 0.016*\"y\" + 0.015*\"see\" + 0.013*\"wouldn\" + 0.013*\"4\" + 0.013*\"fear\" + 0.012*\"ceo\"'), (1, u'0.052*\"trump\" + 0.050*\"session\" + 0.036*\"jeff\" + 0.032*\"right\" + 0.019*\"gener\" + 0.019*\"black\" + 0.019*\"attorney\" + 0.016*\"voter\" + 0.016*\"anti\" + 0.016*\"s\"'), (2, u'0.130*\"trump\" + 0.072*\"settl\" + 0.071*\"univers\" + 0.061*\"million\" + 0.055*\"25\" + 0.042*\"lawsuit\" + 0.039*\"fraud\" + 0.026*\"pay\" + 0.023*\"donald\" + 0.020*\"case\"'), (3, u'0.047*\"trump\" + 0.023*\"s\" + 0.019*\"hope\" + 0.018*\"one\" + 0.018*\"still\" + 0.013*\"work\" + 0.011*\"maga\" + 0.011*\"can\" + 0.009*\"interest\" + 0.009*\"3\"'), (4, u'0.053*\"trump\" + 0.020*\"clinton\" + 0.018*\"s\" + 0.012*\"offici\" + 0.011*\"make\" + 0.011*\"go\" + 0.010*\"sad\" + 0.008*\"leav\" + 0.008*\"yeah\" + 0.008*\"priebu\"'), (5, u'0.066*\"gener\" + 0.061*\"attorney\" + 0.055*\"trump\" + 0.021*\"call\" + 0.018*\"cia\" + 0.018*\"pick\" + 0.017*\"name\" + 0.016*\"via\" + 0.016*\"s\" + 0.015*\"session\"'), (6, u'0.061*\"trump\" + 0.040*\"nation\" + 0.031*\"secur\" + 0.029*\"announc\" + 0.026*\"muslim\" + 0.024*\"reach\" + 0.022*\"advisor\" + 0.019*\"principl\" + 0.019*\"stop\" + 0.014*\"stay\"'), (7, u'0.077*\"trump\" + 0.071*\"get\" + 0.064*\"meet\" + 0.059*\"first\" + 0.057*\"presid\" + 0.053*\"may\" + 0.053*\"grab\" + 0.051*\"discuss\" + 0.048*\"attent\" + 0.048*\"crotch\"'), (8, u'0.055*\"25m\" + 0.046*\"trump\" + 0.039*\"m\" + 0.027*\"s\" + 0.022*\"elect\" + 0.021*\"fuck\" + 0.013*\"bad\" + 0.013*\"000\" + 0.013*\"peopl\" + 0.011*\"donald\"'), (9, u'0.069*\"trump\" + 0.029*\"s\" + 0.020*\"news\" + 0.017*\"presid\" + 0.015*\"say\" + 0.013*\"immigr\" + 0.012*\"mexico\" + 0.011*\"everyon\" + 0.010*\"npr\" + 0.010*\"ivanka\"'), (10, u'0.045*\"trump\" + 0.045*\"racist\" + 0.036*\"s\" + 0.034*\"bannon\" + 0.021*\"mean\" + 0.021*\"steve\" + 0.017*\"c\" + 0.017*\"michael\" + 0.016*\"elect\" + 0.015*\"white\"'), (11, u'0.084*\"vote\" + 0.075*\"trump\" + 0.056*\"kany\" + 0.025*\"obama\" + 0.025*\"didn\" + 0.023*\"realdonaldtrump\" + 0.022*\"ve\" + 0.020*\"west\" + 0.020*\"won\" + 0.016*\"approv\"'), (12, u'0.087*\"trump\" + 0.054*\"agre\" + 0.049*\"break\" + 0.043*\"new\" + 0.035*\"settlement\" + 0.026*\"york\" + 0.025*\"donald\" + 0.023*\"ag\" + 0.020*\"univers\" + 0.018*\"case\"'), (13, u'0.039*\"trump\" + 0.022*\"kany\" + 0.018*\"revers\" + 0.017*\"boycott\" + 0.016*\"ap\" + 0.014*\"suck\" + 0.013*\"bush\" + 0.013*\"s\" + 0.012*\"put\" + 0.011*\"doesn\"'), (14, u'0.064*\"trump\" + 0.036*\"give\" + 0.026*\"money\" + 0.023*\"ford\" + 0.020*\"lie\" + 0.017*\"fan\" + 0.015*\"compani\" + 0.015*\"reuter\" + 0.012*\"destroy\" + 0.011*\"anoth\"'), (15, u'0.066*\"trump\" + 0.032*\"will\" + 0.016*\"presid\" + 0.014*\"year\" + 0.014*\"s\" + 0.012*\"student\" + 0.012*\"elect\" + 0.011*\"school\" + 0.010*\"next\" + 0.010*\"great\"'), (16, u'0.064*\"s\" + 0.063*\"trump\" + 0.026*\"flynn\" + 0.018*\"mike\" + 0.015*\"cabinet\" + 0.012*\"think\" + 0.012*\"don\" + 0.012*\"propos\" + 0.012*\"pompeo\" + 0.011*\"aclu\"'), (17, u'0.061*\"trump\" + 0.025*\"take\" + 0.017*\"just\" + 0.015*\"s\" + 0.015*\"deal\" + 0.015*\"say\" + 0.014*\"stand\" + 0.013*\"white\" + 0.013*\"busi\" + 0.013*\"melania\"'), (18, u'0.089*\"trump\" + 0.060*\"support\" + 0.055*\"starbuck\" + 0.028*\"kany\" + 0.027*\"protest\" + 0.022*\"re\" + 0.020*\"cup\" + 0.017*\"write\" + 0.015*\"buy\" + 0.014*\"y\"'), (19, u'0.041*\"trump\" + 0.020*\"guy\" + 0.016*\"obama\" + 0.016*\"day\" + 0.014*\"can\" + 0.012*\"come\" + 0.011*\"re\" + 0.011*\"class\" + 0.011*\"ll\" + 0.010*\"peopl\"')]\n",
      "[(0, u'0.226*\"hilli\" + 0.095*\"clinton\" + 0.039*\"via\" + 0.028*\"worldstar\" + 0.028*\"iamdanielnwosu\" + 0.028*\"dwb3nfcxpz\" + 0.028*\"ig\" + 0.011*\"russia\" + 0.010*\"part\" + 0.008*\"team\"'), (1, u'0.032*\"foxnew\" + 0.026*\"clinton\" + 0.024*\"free\" + 0.021*\"d\" + 0.021*\"read\" + 0.019*\"can\" + 0.017*\"maga\" + 0.017*\"bro_pair\" + 0.013*\"s\" + 0.013*\"cnn\"'), (2, u'0.054*\"clinton\" + 0.037*\"s\" + 0.024*\"elect\" + 0.020*\"piec\" + 0.020*\"right\" + 0.019*\"con\" + 0.016*\"trump\" + 0.014*\"zzflc0jqsg\" + 0.014*\"rapspotlight\" + 0.012*\"civil\"'), (3, u'0.040*\"one\" + 0.031*\"right\" + 0.022*\"democrat\" + 0.021*\"republican\" + 0.021*\"white\" + 0.021*\"clinton\" + 0.021*\"want\" + 0.019*\"administr\" + 0.018*\"mayb\" + 0.018*\"interest\"'), (4, u'0.076*\"vote\" + 0.071*\"clinton\" + 0.056*\"trump\" + 0.038*\"hillari\" + 0.030*\"donald\" + 0.029*\"just\" + 0.027*\"million\" + 0.021*\"1\" + 0.017*\"popular\" + 0.014*\"presid\"'), (5, u'0.061*\"clinton\" + 0.035*\"bill\" + 0.029*\"s\" + 0.024*\"vote\" + 0.022*\"director\" + 0.022*\"million\" + 0.019*\"2\" + 0.017*\"popular\" + 0.016*\"obama\" + 0.016*\"trump\"'), (6, u'0.034*\"clinton\" + 0.032*\"re\" + 0.030*\"found\" + 0.028*\"investig\" + 0.026*\"dead\" + 0.026*\"monica\" + 0.025*\"haiti\" + 0.022*\"petersen\" + 0.021*\"un\" + 0.021*\"dem\"'), (7, u'0.059*\"clinton\" + 0.056*\"like\" + 0.042*\"hillari\" + 0.039*\"donat\" + 0.037*\"campaign\" + 0.037*\"kany\" + 0.036*\"s\" + 0.029*\"look\" + 0.027*\"west\" + 0.025*\"2015\"'), (8, u'0.087*\"clinton\" + 0.058*\"campaign\" + 0.050*\"s\" + 0.042*\"let\" + 0.037*\"w\" + 0.032*\"start\" + 0.028*\"journalist\" + 0.026*\"collud\" + 0.017*\"probabl\" + 0.016*\"foundat\"'), (9, u'0.076*\"clinton\" + 0.037*\"hillari\" + 0.033*\"santa\" + 0.031*\"list\" + 0.030*\"job\" + 0.029*\"tell\" + 0.028*\"mall\" + 0.028*\"lose\" + 0.028*\"naughti\" + 0.024*\"girl\"'), (10, u'0.030*\"clinton\" + 0.024*\"obama\" + 0.016*\"polit\" + 0.013*\"johnson\" + 0.012*\"trump\" + 0.012*\"last\" + 0.011*\"natesilver538\" + 0.010*\"must\" + 0.010*\"just\" + 0.009*\"rest\"'), (11, u'0.072*\"elector\" + 0.058*\"clinton\" + 0.039*\"colleg\" + 0.039*\"hillari\" + 0.027*\"make\" + 0.024*\"sign\" + 0.024*\"presid\" + 0.019*\"19\" + 0.018*\"trump\" + 0.017*\"stop\"'), (12, u'0.090*\"clinton\" + 0.056*\"hillari\" + 0.042*\"investig\" + 0.041*\"foundat\" + 0.035*\"found\" + 0.033*\"dead\" + 0.032*\"woman\" + 0.026*\"get\" + 0.022*\"crime\" + 0.018*\"youtub\"'), (13, u'0.055*\"clinton\" + 0.021*\"christichat\" + 0.017*\"advertis\" + 0.014*\"hillari\" + 0.014*\"best\" + 0.012*\"call\" + 0.011*\"pizzag\" + 0.011*\"heard\" + 0.011*\"s\" + 0.011*\"honest_hillari\"'), (14, u'0.078*\"clinton\" + 0.061*\"hillari\" + 0.050*\"plan\" + 0.041*\"elect\" + 0.028*\"hous\" + 0.026*\"conserv\" + 0.024*\"pre\" + 0.024*\"trump\" + 0.024*\"pr\" + 0.024*\"dollar\"'), (15, u'0.057*\"clinton\" + 0.056*\"s\" + 0.034*\"trump\" + 0.022*\"new\" + 0.020*\"don\" + 0.016*\"go\" + 0.015*\"hit\" + 0.015*\"presid\" + 0.014*\"hillari\" + 0.014*\"talk\"'), (16, u'0.111*\"news\" + 0.065*\"clinton\" + 0.054*\"fake\" + 0.051*\"network\" + 0.045*\"cernovich\" + 0.020*\"g1xhimp0of\" + 0.020*\"shut\" + 0.019*\"ya\" + 0.018*\"lib\" + 0.018*\"col_nj\"'), (17, u'0.044*\"will\" + 0.042*\"now\" + 0.034*\"clinton\" + 0.030*\"us\" + 0.025*\"peopl\" + 0.023*\"mani\" + 0.020*\"vote\" + 0.020*\"democrat\" + 0.017*\"show\" + 0.016*\"fraud\"'), (18, u'0.069*\"clinton\" + 0.036*\"never\" + 0.030*\"s\" + 0.025*\"hillari\" + 0.020*\"one\" + 0.019*\"love\" + 0.018*\"elect\" + 0.018*\"america\" + 0.018*\"email\" + 0.018*\"see\"'), (19, u'0.062*\"clinton\" + 0.040*\"hillari\" + 0.038*\"gener\" + 0.036*\"attorney\" + 0.030*\"session\" + 0.022*\"patriot\" + 0.019*\"jeff\" + 0.019*\"mean\" + 0.018*\"support\" + 0.014*\"look\"')]\n"
     ]
    }
   ],
   "source": [
    "#Running LDA\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "twitter_words= ['https','co','http','rt','RT','u','t','amp',]\n",
    "en_stop = get_stop_words('en')+twitter_words\n",
    "p_stemmer = PorterStemmer()\n",
    "kywrds = ['Obama','Trump','Clinton']\n",
    "\n",
    "for wrd in kywrds:\n",
    "    infile = open('C:\\\\Users\\\\Tanvi\\\\Downloads\\\\Project\\\\json\\\\{}Tweets.txt'.format(wrd)).readlines()\n",
    "    texts = []\n",
    "\n",
    "    for i in infile:\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        texts.append(stemmed_tokens)\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # generate LDA model\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word = dictionary, passes=20)\n",
    "    print(ldamodel.print_topics())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
